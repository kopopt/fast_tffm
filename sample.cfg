[General]
# The number of features stored in the model. feature_entry_id = feature_original_id % vocabulary_size
vocabulary_size = 50000

# Number of vocabulary blocks. Each vocabulary block holds (vocabulary_size / vocabulary_block_num + 1) entries.
vocabulary_block_num = 2

# The total memory used is vocabulary_size * (factor_num + 1). The "1" is used for saving a bias term for each feature entry.
factor_num = 8

# The file path 1) for saving model when training; 2) for loading model when predicting.
model_file = ./model

# Data file input format: <label> <fid_0>[:<fval_0>] [<fid_1>[:<fval_1>] ... ]
# If hash_feature_id = True, <fid_k> must be an integer; otherwise, the <fid_k> string will be hashed to an int64
hash_feature_id = True

[ClusterSpec]
# Hosts used as parameter servers. Seperated by comma. 
# Note: vocabulary_block_num >= len(ps_hosts) should be satisfied; otherwise some parameter servers are idle.  
ps_hosts = localhost:2340,localhost:2341

# Hosts used as workers. Seperated by comma.
# Note: Each worker will retrieve one whole file for training/prediction. It is suggested that len(train_files) > len(worker_hosts)
worker_hosts = localhost:2342,localhost:2343

[Train]
batch_size = 100
init_value_range = 0.0001
factor_lambda = 1
bias_lambda = 1
thread_num = 10
epoch_num = 2
learning_rate = 0.01
adagrad.initial_accumulator = 0.1

# logistic or mse. Use logistic loss (binary) or mean squared error.
loss_type = logistic

# Seperated by comma, e.g., file1,file2,...
train_files = ./data/train_0,./data/train_1,./data/train_2,./data/train_3,./data/train_4

# Optional. Seperated by comma. Each weight file corresond to a train file and should have the same line number. If not provided, assume weight = 1 for all examples.
weight_files = ./data/weight_0,./data/weight_1,./data/weight_2,./data/weight_3,./data/weight_4

# Optional. Seperated by comma. If not provided, skip the validation phase.
validation_files = ./data/test_0,./data/test_1

[Predict]
predict_files = ./data/test_0,./data/test_1
score_path = ./score/
